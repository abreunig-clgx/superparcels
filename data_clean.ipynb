{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "from shapely import wkt\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = r'D:\\Projects\\superparcels\\data\\abreunig_pocs_spatialrecord_polygon_superparcel_urban.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'D:\\Projects\\superparcels\\data\\Urban'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path, dtype={'FIPS': str})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "fips = df['FIPS'].unique()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['06075', '08031', '06001', '48113'], dtype=object)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fips"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "06075\n",
      "08031\n",
      "06001\n",
      "48113\n"
     ]
    }
   ],
   "source": [
    "for fip in fips:\n",
    "    print(fip)\n",
    "    df_fip = df[df['FIPS'] == fip]\n",
    "    gdf = gpd.GeoDataFrame(df_fip, geometry=df_fip['geometry'].apply(wkt.loads), crs=4326)\n",
    "    gdf.to_file(os.path.join(data_dir, f'sp_sample_{fip}.shp'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Clean parcel urban and remove duplicate geometries for clustering POC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir = r'D:\\Projects\\superparcels\\data\\Urban'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fi in glob.glob(os.path.join(input_dir, '*\\*.shp')):\n",
    "    fips = os.path.basename(fi).split('_')[-1].split('.')[0]\n",
    "    df = gpd.read_file(fi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "classification\n",
      "Class6: Unique Owner & Address & Geometry              14822\n",
      "Class5: Unique Owner & Address, Duplicate Geometry      5787\n",
      "Class1: Duplicate Owner, Address & Geometry             3211\n",
      "Class3: Duplicate Owner & Unique Address, Geometry      2111\n",
      "Class2: Duplicate Owner & Address, Unique Geometry       434\n",
      "Class4: Unique Owner & Duplicate Address & Geometry       16\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Identify duplicate owners, addresses, and geometries\n",
    "df['duplicate_owner'] = df.duplicated(subset=['OWNER'], keep=False)\n",
    "df['duplicate_address'] = df.duplicated(subset=['std_addr'], keep=False)\n",
    "df['duplicate_geometry'] = df.duplicated(subset=['geometry'], keep=False)\n",
    "\n",
    "# Create a classification column based on duplication status (with geometry)\n",
    "df['classification'] = df.apply(\n",
    "    lambda row: (\n",
    "        'Class1: Duplicate Owner, Address & Geometry' if row['duplicate_owner'] and row['duplicate_address'] and row['duplicate_geometry'] else\n",
    "        'Class2: Duplicate Owner & Address, Unique Geometry' if row['duplicate_owner'] and row['duplicate_address'] and not row['duplicate_geometry'] else\n",
    "        'Class3: Duplicate Owner & Unique Address, Geometry' if row['duplicate_owner'] and not row['duplicate_address'] and not row['duplicate_geometry'] else\n",
    "        'Class4: Unique Owner & Duplicate Address & Geometry' if not row['duplicate_owner'] and row['duplicate_address'] and row['duplicate_geometry'] else\n",
    "        'Class5: Unique Owner & Address, Duplicate Geometry' if not row['duplicate_owner'] and not row['duplicate_address'] and row['duplicate_geometry'] else\n",
    "        'Class6: Unique Owner & Address & Geometry'\n",
    "    ), axis=1\n",
    ")\n",
    "\n",
    "# Count the occurrences of each classification\n",
    "matrix = df['classification'].value_counts()\n",
    "\n",
    "print(matrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "classification_code\n",
       "6    14822\n",
       "5     5787\n",
       "1     3211\n",
       "3     2111\n",
       "2      434\n",
       "4       16\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create clasification codes\n",
    "df['classification_code'] = df['classification'].apply(lambda x: x.split(':')[0][5])\n",
    "df['classification_code'] = df['classification_code'].astype(int)\n",
    "df['classification_code'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_code = 2\n",
    "code = df[df['classification_code'] == class_code]\n",
    "code.to_file(os.path.join(input_dir, f'sp_sample_08013_code{class_code}.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_canidate_codes = [2,3] # classes after dissolving\n",
    "cluster = df[df['classification_code'].isin(cluster_canidate_codes)]\n",
    "cluster.to_file(os.path.join(input_dir, f'sp_sample_08013_cluster_canidates.shp'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
