{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geometry import *\n",
    "\n",
    "from shapely.ops import nearest_points\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from scipy.ndimage import uniform_filter1d\n",
    "import numpy as np\n",
    "from math import ceil\n",
    "from sklearn.cluster import DBSCAN\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MRE Data: County and Place Data\n",
    "Place boundaries represent locally dense urban areas. Boundaries will be used initally as a proxy for regional parcel density where adaptive eps will be calcualted. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "county1 = mPolygon(origin=(0,0), size=(25,25), alpha=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "place1 = mPolygon(origin=(0, 0), size=(10, 10))\n",
    "place2 = mPolygon(origin=(0, 12), size=(10, 10))\n",
    "place3 = mPolygon(origin=(14, 5), size=(10, 10))\n",
    "places = [place1, place2, place3]\n",
    "\n",
    "place_data = {\n",
    "    'Place': ['P1', 'P2', 'P3']\n",
    "}\n",
    "place_gdf = GeoDataFrame(place_data, places).build()\n",
    "place_gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map = Map()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map.add_gdf(place_gdf, label='Place', alpha=0)\n",
    "map.add_shape(county1)\n",
    "map.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places 1 Parcel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_1 = mPolygon(origin=(0, 0), size=(2,2))\n",
    "p1_2 = mPolygon(origin=(0,2), size=(2,2))\n",
    "p1_3 = mPolygon(origin=(0,4), size=(2,2))\n",
    "p1_4 = mPolygon(origin=(0,6), size=(2,2))\n",
    "p1_5 = mPolygon(origin=(0,8), size=(2,2))\n",
    "p1_6 = mPolygon(origin=(2,0), size=(2,2))\n",
    "p1_7 = mPolygon(origin=(2,2), size=(2,2))\n",
    "p1_8 = mPolygon(origin=(2,4), size=(2,2))\n",
    "p1_9 = mPolygon(origin=(2,6), size=(2,2))\n",
    "p1_10 = mPolygon(origin=(2,8), size=(2,2))\n",
    "\n",
    "p1_11 = mPolygon(origin=(6,0), size=(2,2))\n",
    "p1_12 = mPolygon(origin=(6,2), size=(2,2))\n",
    "p1_13 = mPolygon(origin=(6,4), size=(2,2))\n",
    "p1_14 = mPolygon(origin=(6,6), size=(2,2))\n",
    "p1_15 = mPolygon(origin=(6,8), size=(2,2))\n",
    "p1_16 = mPolygon(origin=(8,0), size=(2,2))\n",
    "p1_17 = mPolygon(origin=(8,2), size=(2,2))\n",
    "p1_18 = mPolygon(origin=(8,4), size=(2,2))\n",
    "p1_19 = mPolygon(origin=(8,6), size=(2,2))\n",
    "p1_20 = mPolygon(origin=(8,8), size=(2,2))\n",
    "\n",
    "p1_parcels = [p1_1, p1_2, p1_3, p1_4, p1_5, p1_6, p1_7, p1_8, p1_9, p1_10, p1_11, p1_12, p1_13, p1_14, p1_15, p1_16, p1_17, p1_18, p1_19, p1_20]\n",
    "\n",
    "p1_parcel_data = {\n",
    "    'OWNER': ['D', 'D', 'F', 'E', 'E', 'A', 'D', 'E', 'E', 'E', 'A', 'D', 'D', 'C', 'F', 'B', 'B', 'C', 'F', 'F']\n",
    "}\n",
    "p1_parcel_gdf = GeoDataFrame(p1_parcel_data, p1_parcels).build()\n",
    "\n",
    "# add line to map\n",
    "p1_line = mLine(coords=[(5, 0), (5, 10)], color='black')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map.add_gdf(p1_parcel_gdf, label='OWNER', alpha=0.2, color='blue')\n",
    "map.add_shape(p1_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Places 2 Parcel Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p2_1 = mPolygon(origin=(0, 12), size=(2,2))\n",
    "p2_2 = mPolygon(origin=(0,14), size=(2,2))\n",
    "p2_3 = mPolygon(origin=(0,16), size=(2,2))\n",
    "p2_4 = mPolygon(origin=(0,18), size=(2,2))\n",
    "p2_5 = mPolygon(origin=(0,20), size=(2,2))\n",
    "p2_6 = mPolygon(origin=(2,12), size=(2,2))\n",
    "p2_7 = mPolygon(origin=(2,14), size=(2,2))\n",
    "p2_8 = mPolygon(origin=(2,16), size=(2,2))\n",
    "p2_9 = mPolygon(origin=(2,18), size=(2,2))\n",
    "p2_10 = mPolygon(origin=(2,20), size=(2,2))\n",
    "\n",
    "p2_11 = mPolygon(origin=(6,12), size=(2,2))\n",
    "p2_12 = mPolygon(origin=(6,14), size=(2,2))\n",
    "p2_13 = mPolygon(origin=(6,16), size=(2,2))\n",
    "p2_14 = mPolygon(origin=(6,18), size=(2,2))\n",
    "p2_15 = mPolygon(origin=(6,20), size=(2,2))\n",
    "p2_16 = mPolygon(origin=(8,12), size=(2,2))\n",
    "p2_17 = mPolygon(origin=(8,14), size=(2,2))\n",
    "p2_18 = mPolygon(origin=(8,16), size=(2,2))\n",
    "p2_19 = mPolygon(origin=(8,18), size=(2,2))\n",
    "p2_20 = mPolygon(origin=(8,20), size=(2,2))\n",
    "\n",
    "\n",
    "p2_parcels = [p2_1, p2_2, p2_3, p2_4, p2_5, p2_6, p2_7, p2_8, p2_9, p2_10, p2_11, p2_12, p2_13, p2_14, p2_15, p2_16, p2_17, p2_18, p2_19, p2_20]\n",
    "\n",
    "p2_parcel_data = {\n",
    "    'OWNER': ['E', 'C', 'C', 'C', 'E', 'E', 'B', 'F', 'A', 'D', 'F', 'F', 'C', 'C', 'C', 'F', 'G', 'H', 'I', 'J']\n",
    "}\n",
    "p2_parcel_gdf = GeoDataFrame(p2_parcel_data, p2_parcels).build()\n",
    "\n",
    "# add line to map\n",
    "p2_line = mLine(coords=[(5, 12), (5, 22)], color='black')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map.add_gdf(p2_parcel_gdf, label='OWNER', alpha=0.2, color='blue')\n",
    "map.add_shape(p2_line)\n",
    "map.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clustering Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def polygon_distance(polygon1, polygon2):\n",
    "    # Calculate the minimum distance between two polygons\n",
    "    point1, point2 = nearest_points(polygon1, polygon2)\n",
    "    return point1.distance(point2)\n",
    "\n",
    "def compute_distance_matrix(polygons):\n",
    "    # Create a distance matrix between all polygons\n",
    "    num_polygons = len(polygons)\n",
    "    distance_matrix = np.zeros((num_polygons, num_polygons))\n",
    "    \n",
    "    for i in range(num_polygons):\n",
    "        for j in range(i + 1, num_polygons):\n",
    "            distance_matrix[i, j] = polygon_distance(polygons[i], polygons[j])\n",
    "            distance_matrix[j, i] = distance_matrix[i, j]  # Symmetry\n",
    "    \n",
    "    return distance_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcels = pd.concat([p1_parcel_gdf, p2_parcel_gdf], ignore_index=True)\n",
    "\n",
    "display(parcels.head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 3\n",
    "min_urban_distance = 3\n",
    "max_distance = 5\n",
    "for place_id, place_data in place_gdf.iterrows():\n",
    "    print(f\"Processing place {place_id}\")\n",
    "    sub_parcels = parcels[parcels.within(place_data['geometry'])]\n",
    "    \n",
    "    def compute_regional_distance_matrix(df):\n",
    "        distance_matrix = df.geometry.apply(lambda g: df.distance(g)).values\n",
    "        distances = distance_matrix[np.triu_indices_from(distance_matrix, k=1)]\n",
    "        distances = distances[distances > 0]\n",
    "\n",
    "        return distances\n",
    "\n",
    "    distances = compute_regional_distance_matrix(sub_parcels)\n",
    "\n",
    "    def compute_nneighbors(dmatrix, ratio, max_neighbors):\n",
    "        # 10% of the data or 5 neighbors\n",
    "        n_neighbors = max(int(len(dmatrix) * ratio), max_neighbors)\n",
    "        return n_neighbors\n",
    "\n",
    "    n_neighbors = compute_nneighbors(distances, ratio=0.1, max_neighbors=5)\n",
    "    \n",
    "    def compute_optimal_distance(dmatrix, n_neighbors, min_urban_distance, max_distance):\n",
    "        knn = NearestNeighbors(n_neighbors=n_neighbors).fit(dmatrix.reshape(-1, 1))\n",
    "        knn_distances, _ = knn.kneighbors(dmatrix.reshape(-1, 1))\n",
    "        sorted_distances = np.sort(knn_distances[:, -1])\n",
    "        smooth_dist = uniform_filter1d(sorted_distances, size=10)\n",
    "        difference = np.diff(smooth_dist)\n",
    "        elbow_index = np.argmax(difference) + 1\n",
    "\n",
    "        # take distance from KNN elbow --> must be greater than min_urban_distance and less than max_distance\n",
    "        optimal_distance = min(max(ceil(smooth_dist[elbow_index]), min_urban_distance), max_distance)\n",
    "        \n",
    "        return optimal_distance\n",
    "\n",
    "    optimal_distance = compute_optimal_distance(distances, n_neighbors, min_urban_distance, max_distance)\n",
    "    \n",
    "    \n",
    "    unique_owners = sub_parcels['OWNER'].unique()\n",
    "    clustered_parcel_data = gpd.GeoDataFrame()\n",
    "    single_parcel_data = gpd.GeoDataFrame()\n",
    "    for owner in unique_owners:\n",
    "        owner_parcels = sub_parcels[sub_parcels['OWNER'] == owner]\n",
    "        print(f\"Owner {owner} has {len(owner_parcels)} parcels\")\n",
    "        polygons = owner_parcels['geometry'].to_list()\n",
    "        distance_matrix = compute_distance_matrix(polygons)\n",
    "        if distance_matrix.shape[0] < 3: # only two parcels\n",
    "            continue\n",
    "\n",
    "        dbscan = DBSCAN(eps=optimal_distance, min_samples=sample_size, metric='precomputed')\n",
    "        clusters = dbscan.fit_predict(distance_matrix)\n",
    "\n",
    "        owner_parcels['cluster'] = clusters # clustert ID\n",
    "        owner_parcels['area'] = owner_parcels['geometry'].area\n",
    "        counts = owner_parcels['cluster'].value_counts() # pd.series of cluster counts\n",
    "        \n",
    "        outliers = counts[counts.index == -1].index # outliers always identified as -1\n",
    "        counts = counts[counts.index != -1] # drop outliers\n",
    "\n",
    "        single_parcel_filter_ids = set(list(outliers)) # not apart of any cluster\n",
    "            \n",
    "        single_parcel_filter = owner_parcels[owner_parcels['cluster'].isin(single_parcel_filter_ids)]\n",
    "        single_parcel_data = pd.concat([single_parcel_data, single_parcel_filter], ignore_index=True)\n",
    "        \n",
    "        cluster_filter = owner_parcels[~owner_parcels['cluster'].isin(single_parcel_filter_ids)]\n",
    "        if len(cluster_filter) > 0:\n",
    "            cluster_filter['pcount'] = cluster_filter['cluster'].map(counts) # add parcel count to filter dataframe\n",
    "            cluster_filter['buff_dist'] = optimal_distance # dbscan distane is euivalent to the required buffer distance\n",
    "            clustered_parcel_data = pd.concat([clustered_parcel_data, cluster_filter], ignore_index=True)\n",
    "\n",
    "        \n",
    "\n",
    "    \n",
    "\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min(max(100, 15), 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustered_parcel_data['cluster_ID'] = clustered_parcel_data['OWNER'] + '_' + clustered_parcel_data['cluster'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map = Map()\n",
    "cluster_map.add_gdf(clustered_parcel_data, label='cluster_ID', alpha=0.2, color='blue')\n",
    "cluster_map.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_dissolve = clustered_parcel_data.dissolve(by='cluster_ID').reset_index()\n",
    "parcel_dissolve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parcel_dissolve['geometry'] = parcel_dissolve['geometry'].buffer(3)\n",
    "parcel_dissolve['geometry'] = parcel_dissolve['geometry'].buffer(-3)\n",
    "parcel_dissolve = parcel_dissolve.explode(index_parts=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_map.add_gdf(parcel_dissolve, alpha=0, edgecolor='red', label='OWNER', label_color='red')\n",
    "cluster_map.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map.add_gdf(parcel_dissolve, alpha=0, edgecolor='red', label='OWNER', label_color='red')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
